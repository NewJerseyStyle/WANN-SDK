# WANN SDK - Developer Documentation

å®Œæ•´çš„ Weight Agnostic Neural Networks SDK,æ”¯æŒæ¶æ§‹æœç´¢å’Œå¤šç¨® RL è¨“ç·´æ–¹æ³•ã€‚

## ğŸ“š ç›®éŒ„

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [æ ¸å¿ƒæ¶æ§‹](#æ ¸å¿ƒæ¶æ§‹)
3. [å®‰è£æŒ‡å—](#å®‰è£æŒ‡å—)
4. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
5. [æ ¸å¿ƒçµ„ä»¶](#æ ¸å¿ƒçµ„ä»¶)
6. [æ·»åŠ æ–°è¨“ç·´æ–¹æ³•](#æ·»åŠ æ–°è¨“ç·´æ–¹æ³•)
7. [åˆ†ä½ˆå¼è¨“ç·´](#åˆ†ä½ˆå¼è¨“ç·´)
8. [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
9. [API åƒè€ƒ](#api-åƒè€ƒ)

## ğŸ¯ æ¦‚è¿°

### è¨­è¨ˆç†å¿µ

WANN SDK éµå¾ªä»¥ä¸‹è¨­è¨ˆåŸå‰‡:

1. **é—œæ³¨é»åˆ†é›¢**: æ¶æ§‹æœç´¢èˆ‡æ¬Šé‡è¨“ç·´è§£è€¦
2. **è¨“ç·´æ–¹æ³•ç„¡é—œ**: æ”¯æŒä»»æ„ RL ç®—æ³• (DQN, PPO, SAC, ...)
3. **ç’°å¢ƒç„¡é—œ**: å…¼å®¹æ‰€æœ‰ Gymnasium ç’°å¢ƒ
4. **å¯æ“´å±•æ€§**: æ˜“æ–¼æ·»åŠ æ–°æ–¹æ³•å’Œçµ„ä»¶
5. **åˆ†ä½ˆå¼å„ªå…ˆ**: å…§å»º Ray æ”¯æŒ

### æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WANN SDK Architecture                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Architecture     â”‚         â”‚ Training Methods  â”‚   â”‚
â”‚  â”‚ Search (WANN)    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ - DQN             â”‚   â”‚
â”‚  â”‚                  â”‚         â”‚ - PPO             â”‚   â”‚
â”‚  â”‚ - TensorNEAT     â”‚         â”‚ - SAC             â”‚   â”‚
â”‚  â”‚ - EvoX           â”‚         â”‚ - Custom...       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                            â”‚               â”‚
â”‚           â”‚                            â”‚               â”‚
â”‚           â–¼                            â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Architecture     â”‚         â”‚ Policy Interface  â”‚   â”‚
â”‚  â”‚ Specification    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                            â”‚               â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                        â–¼                               â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚           â”‚  Environment Service   â”‚                   â”‚
â”‚           â”‚  - Gymnasium Wrapper   â”‚                   â”‚
â”‚           â”‚  - Ray Remote          â”‚                   â”‚
â”‚           â”‚  - Ray Serve API       â”‚                   â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ å®‰è£æŒ‡å—

### åŸºç¤å®‰è£

```bash
# 1. JAX
pip install -U "jax[cuda12]"  # GPU
# æˆ–
pip install -U jax             # CPU

# 2. TensorNEAT
pip install git+https://github.com/EMI-Group/tensorneat.git

# 3. Gymnasium
pip install gymnasium[box2d]

# 4. å…¶ä»–ä¾è³´
pip install optax
```

### åˆ†ä½ˆå¼è¨“ç·´ (å¯é¸)

```bash
# Ray for distributed computing
pip install "ray[serve]"
```

### EvoX é›†æˆ (å¯é¸)

```bash
# JAX-based EvoX
pip install "git+https://github.com/EMI-Group/evox@v0.9.1-dev"
```

### é©—è­‰å®‰è£

```bash
python -c "
import jax
import tensorneat
import gymnasium
import ray
print('All dependencies installed!')
"
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æœ€å°ç¤ºä¾‹ - CartPole

```python
from wann_sdk_core import (
    ArchitectureSpec,
    WANNArchitecture,
    TrainingConfig,
    create_trainer_from_checkpoint
)
from wann_sdk_ray_env import EnvFactory

# 1. å‡è¨­å·²æœ‰æ¶æ§‹ (ä¾†è‡ªæœç´¢éšæ®µ)
spec = ArchitectureSpec.load('./models/cartpole_arch.pkl')
architecture = WANNArchitecture(spec)

# 2. å‰µå»ºç’°å¢ƒå·¥å» 
env_factory = EnvFactory(env_name="CartPole-v1", mode="local")

# 3. å‰µå»ºè¨“ç·´å™¨
config = TrainingConfig(
    learning_rate=1e-3,
    batch_size=128,
    num_epochs=100
)

trainer = create_trainer_from_checkpoint(
    checkpoint_path='./models/cartpole_arch.pkl',
    training_method='dqn',
    env_factory=env_factory.create,
    config=config
)

# 4. è¨“ç·´
trainer.train(num_steps=50000)

print("Training completed!")
```

### BipedalWalker å®Œæ•´æµç¨‹

```bash
# 1. æ¶æ§‹æœç´¢
python wann_bipedal.py --mode search \
    --pop_size 1000 \
    --generations 100 \
    --workers 4

# 2. æ¬Šé‡è¨“ç·´
python wann_bipedal.py --mode train \
    --method ppo \
    --steps 1000000 \
    --distributed

# 3. è©•ä¼°
python wann_bipedal.py --mode eval \
    --render \
    --eval_episodes 10
```

## ğŸ§© æ ¸å¿ƒçµ„ä»¶

### 1. æ¶æ§‹è¦ç¯„ (ArchitectureSpec)

ä¿å­˜å¾æ¶æ§‹æœç´¢éšæ®µå¾—åˆ°çš„ç¶²çµ¡çµæ§‹:

```python
from wann_sdk_core import ArchitectureSpec

spec = ArchitectureSpec(
    nodes=nodes_array,           # ç¯€é»é…ç½®
    connections=conns_array,     # é€£æ¥é…ç½®
    num_inputs=24,
    num_outputs=4,
    num_hidden=15,
    num_params=87,
    search_fitness=250.0,
    search_complexity=87,
    metadata={'env': 'BipedalWalker-v3'}
)

# ä¿å­˜
spec.save('./my_architecture.pkl')

# åŠ è¼‰
spec = ArchitectureSpec.load('./my_architecture.pkl')
```

### 2. WANN æ¶æ§‹ (WANNArchitecture)

å°‡æ¶æ§‹è¦ç¯„è½‰æ›ç‚ºå¯è¨“ç·´çš„ç¶²çµ¡:

```python
from wann_sdk_core import WANNArchitecture

architecture = WANNArchitecture(spec)

# åˆå§‹åŒ–åƒæ•¸
key = jax.random.PRNGKey(42)
params = architecture.init_params(key)

# å‰å‘å‚³æ’­
import jax.numpy as jnp
observation = jnp.zeros(24)
output = architecture.forward(observation, params)

# ç²å–æ¶æ§‹ä¿¡æ¯
info = architecture.get_architecture_info()
print(info)
# {
#     'num_inputs': 24,
#     'num_outputs': 4,
#     'num_hidden': 15,
#     'num_connections': 87,
#     'num_params': 87,
#     'search_fitness': 250.0
# }
```

### 3. ç­–ç•¥æ¥å£ (PolicyInterface)

æ‰€æœ‰è¨“ç·´æ–¹æ³•å¿…é ˆå¯¦ç¾çš„æ¥å£:

```python
from wann_sdk_core import PolicyInterface

class MyCustomPolicy(PolicyInterface):
    def select_action(self, observation, deterministic=False):
        """é¸æ“‡å‹•ä½œ"""
        pass
    
    def compute_loss(self, batch):
        """è¨ˆç®—æå¤±"""
        pass
    
    def update(self, batch):
        """æ›´æ–°åƒæ•¸"""
        pass
    
    def get_params(self):
        """ç²å–åƒæ•¸"""
        pass
    
    def set_params(self, params):
        """è¨­ç½®åƒæ•¸"""
        pass
```

### 4. ç’°å¢ƒåŒ…è£å™¨ (GymnasiumEnvWrapper)

æ¨™æº–åŒ– Gymnasium ç’°å¢ƒæ¥å£:

```python
from wann_sdk_ray_env import GymnasiumEnvWrapper

env = GymnasiumEnvWrapper("BipedalWalker-v3")

# ç²å–ç’°å¢ƒä¿¡æ¯
info = env.get_env_info()
print(info)
# {
#     'env_name': 'BipedalWalker-v3',
#     'obs_dim': 24,
#     'action_dim': 4,
#     'obs_is_discrete': False,
#     'action_is_discrete': False,
#     ...
# }

# ä½¿ç”¨ç’°å¢ƒ
obs, info = env.reset()
action = jnp.array([0.5, -0.3, 0.2, 0.1])
obs, reward, terminated, truncated, info = env.step(action)
```

## ğŸ”§ æ·»åŠ æ–°è¨“ç·´æ–¹æ³•

### æ­¥é©Ÿ 1: å¯¦ç¾ç­–ç•¥æ¥å£

```python
from wann_sdk_core import PolicyInterface, TrainingMethodRegistry
import optax
import jax.numpy as jnp

@TrainingMethodRegistry.register('my_method')
class MyMethodPolicy(PolicyInterface):
    """
    è‡ªå®šç¾©è¨“ç·´æ–¹æ³•ã€‚
    """
    
    def __init__(
        self,
        architecture,
        config,
        **kwargs
    ):
        self.architecture = architecture
        self.config = config
        
        # åˆå§‹åŒ–åƒæ•¸
        key = jax.random.PRNGKey(42)
        self.params = architecture.init_params(key)
        
        # å„ªåŒ–å™¨
        self.optimizer = optax.adam(config.learning_rate)
        self.opt_state = self.optimizer.init(self.params)
        
        # å…¶ä»–çµ„ä»¶ (replay buffer, etc.)
        # ...
    
    def select_action(self, observation, deterministic=False):
        """å¯¦ç¾å‹•ä½œé¸æ“‡é‚è¼¯"""
        output = self.architecture.forward(observation, self.params)
        
        if deterministic:
            return output  # æˆ–å…¶ä»–è™•ç†
        else:
            # æ·»åŠ æ¢ç´¢å™ªè²
            noise = jax.random.normal(key, output.shape) * 0.1
            return output + noise
    
    def compute_loss(self, batch):
        """å¯¦ç¾æå¤±è¨ˆç®—"""
        observations = batch['observations']
        actions = batch['actions']
        rewards = batch['rewards']
        
        # ä½ çš„æå¤±å‡½æ•¸
        predictions = self.architecture.forward(observations, self.params)
        loss = jnp.mean((predictions - actions) ** 2)
        
        return loss
    
    def update(self, batch):
        """å¯¦ç¾åƒæ•¸æ›´æ–°"""
        loss, grads = jax.value_and_grad(self.compute_loss)(batch)
        
        updates, self.opt_state = self.optimizer.update(
            grads, self.opt_state
        )
        self.params = optax.apply_updates(self.params, updates)
        
        return {'loss': float(loss)}
    
    def store_transition(self, obs, action, reward, next_obs, done):
        """å­˜å„²è½‰æ› (å¦‚æœä½¿ç”¨ replay buffer)"""
        # ä½ çš„å¯¦ç¾
        pass
    
    def ready_to_update(self):
        """æª¢æŸ¥æ˜¯å¦æº–å‚™å¥½æ›´æ–°"""
        return True  # æˆ–åŸºæ–¼ buffer å¤§å°ç­‰æ¢ä»¶
    
    def update_step(self):
        """åŸ·è¡Œä¸€æ­¥æ›´æ–°"""
        # å¾ buffer æ¡æ¨£ä¸¦æ›´æ–°
        return self.update(batch)
    
    def get_params(self):
        return self.params
    
    def set_params(self, params):
        self.params = params
```

### æ­¥é©Ÿ 2: ä½¿ç”¨æ–°æ–¹æ³•

```python
from wann_sdk_core import create_trainer_from_checkpoint

# ç¾åœ¨å¯ä»¥ä½¿ç”¨ä½ çš„æ–°æ–¹æ³•
trainer = create_trainer_from_checkpoint(
    checkpoint_path='./arch.pkl',
    training_method='my_method',  # ä½ è¨»å†Šçš„åå­—
    env_factory=env_factory,
    config=config
)

trainer.train(num_steps=100000)
```

### æ­¥é©Ÿ 3: æ·»åŠ æ–¹æ³•ç‰¹å®šåŠŸèƒ½

```python
# ä¾‹å¦‚: æ·»åŠ å„ªå…ˆç´šç¶“é©—å›æ”¾
class MyMethodWithPER(MyMethodPolicy):
    def __init__(self, architecture, config, **kwargs):
        super().__init__(architecture, config, **kwargs)
        
        # å„ªå…ˆç´šç¶“é©—å›æ”¾
        self.per_buffer = PrioritizedReplayBuffer(
            capacity=config.buffer_size,
            alpha=0.6,
            beta=0.4
        )
    
    def store_transition(self, obs, action, reward, next_obs, done):
        # è¨ˆç®— TD error ä½œç‚ºå„ªå…ˆç´š
        td_error = self._compute_td_error(obs, action, reward, next_obs, done)
        self.per_buffer.add(obs, action, reward, next_obs, done, td_error)
```

## ğŸŒ åˆ†ä½ˆå¼è¨“ç·´

### Ray Remote ç’°å¢ƒ

ä½¿ç”¨å¤šå€‹ worker ä¸¦è¡Œæ”¶é›†ç¶“é©—:

```python
from wann_sdk_ray_env import DistributedEnvPool

# å‰µå»ºåˆ†ä½ˆå¼ç’°å¢ƒæ± 
pool = DistributedEnvPool(
    env_name="BipedalWalker-v3",
    num_workers=8  # 8 å€‹ä¸¦è¡Œ worker
)

# å®šç¾©ç­–ç•¥å‡½æ•¸
def policy_fn(params, obs):
    return architecture.forward(obs, params)

# ä¸¦è¡Œæ”¶é›† rollouts
rollouts = pool.collect_rollouts(
    policy_fn=policy_fn,
    policy_params=params,
    num_rollouts=100,
    max_steps=1000
)

# è™•ç†çµæœ
for rollout in rollouts:
    print(f"Worker {rollout['worker_id']}: "
          f"Reward={rollout['episode_reward']:.2f}")

# æ¸…ç†
pool.close_all()
```

### Ray Serve API

å°‡ç’°å¢ƒä½œç‚ºæœå‹™é‹è¡Œ:

```python
from wann_sdk_ray_env import start_environment_service

# å•Ÿå‹•ç’°å¢ƒæœå‹™
start_environment_service(
    env_name="BipedalWalker-v3",
    port=8000
)

# æœå‹™ç«¯é»:
# POST /env/reset - é‡ç½®ç’°å¢ƒ
# POST /env/step - åŸ·è¡Œå‹•ä½œ
# GET  /env/info - ç²å–ç’°å¢ƒä¿¡æ¯
# POST /env/close - é—œé–‰ç’°å¢ƒ
```

å®¢æˆ¶ç«¯ä½¿ç”¨:

```python
import requests

# é‡ç½®
response = requests.post(
    "http://localhost:8000/env/reset",
    json={"seed": 42}
)
obs = response.json()['observation']

# æ­¥é€²
response = requests.post(
    "http://localhost:8000/env/step",
    json={"action": [0.5, -0.3, 0.2, 0.1]}
)
result = response.json()
next_obs = result['observation']
reward = result['reward']
```

### åˆ†ä½ˆå¼æ¶æ§‹æœç´¢

ä½¿ç”¨ EvoX é€²è¡Œå¤šè¨­å‚™æ¶æ§‹æœç´¢:

```python
from wann_evox_adapter import EvoXWANNAlgorithm

# å‰µå»º WANN ç®—æ³•
wann = WANN(pop_size=1024, genome=genome)

# åŒ…è£ç‚º EvoX ç®—æ³•
distributed_config = {
    'num_devices': 4,
    'device_type': 'gpu'
}

algorithm = EvoXWANNAlgorithm(
    wann_algorithm=wann,
    distributed_config=distributed_config
)

# è‡ªå‹•åˆ†ä½ˆå¼è¨“ç·´
for gen in range(num_generations):
    state, distributed_pop = algorithm.distributed_ask(state)
    fitness = problem.distributed_evaluate(state, distributed_pop)
    state = algorithm.distributed_tell(state, fitness)
```

## ğŸ“– å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: CartPole with DQN

```python
"""
CartPole ç’°å¢ƒä½¿ç”¨ DQN è¨“ç·´
"""

import jax
import jax.numpy as jnp
from wann_sdk_core import *
from wann_sdk_ray_env import *
from wann_sdk_rl_methods import *

# 1. æ¶æ§‹æœç´¢ (æˆ–åŠ è¼‰å·²æœ‰æ¶æ§‹)
# ... (architecture search code) ...

# 2. åŠ è¼‰æ¶æ§‹
spec = ArchitectureSpec.load('./cartpole_arch.pkl')
architecture = WANNArchitecture(spec)

# 3. å‰µå»ºç’°å¢ƒ
env = GymnasiumEnvWrapper("CartPole-v1")
env_info = env.get_env_info()

# 4. å‰µå»º DQN ç­–ç•¥
config = TrainingConfig(
    learning_rate=1e-3,
    batch_size=128,
    buffer_size=10000
)

policy = create_policy_for_environment(
    architecture=architecture,
    env_info=env_info,
    method='dqn',
    config=config
)

# 5. è¨“ç·´å¾ªç’°
num_steps = 50000
obs, _ = env.reset()

for step in range(num_steps):
    # é¸æ“‡å‹•ä½œ
    action = policy.select_action(obs)
    
    # åŸ·è¡Œ
    next_obs, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    
    # å­˜å„²
    policy.store_transition(obs, action, reward, next_obs, done)
    
    # æ›´æ–°
    if policy.ready_to_update():
        metrics = policy.update_step()
        
        if step % 1000 == 0:
            print(f"Step {step}: Loss={metrics['loss']:.4f}")
    
    # é‡ç½®
    if done:
        obs, _ = env.reset()
    else:
        obs = next_obs

# 6. ä¿å­˜
final_params = policy.get_params()
# ... save params ...

print("Training completed!")
```

### ç¤ºä¾‹ 2: BipedalWalker with PPO (åˆ†ä½ˆå¼)

```python
"""
BipedalWalker ä½¿ç”¨ PPO å’Œåˆ†ä½ˆå¼è¨“ç·´
"""

# 1. å‰µå»ºåˆ†ä½ˆå¼ç’°å¢ƒæ± 
pool = DistributedEnvPool(
    env_name="BipedalWalker-v3",
    num_workers=8
)

# 2. åŠ è¼‰æ¶æ§‹
spec = ArchitectureSpec.load('./bipedal_arch.pkl')
architecture = WANNArchitecture(spec)

# 3. å‰µå»º PPO ç­–ç•¥
env_info = pool.get_env_info()
config = TrainingConfig(
    learning_rate=3e-4,
    batch_size=256
)

policy = create_policy_for_environment(
    architecture=architecture,
    env_info=env_info,
    method='ppo',
    config=config
)

# 4. è¨“ç·´
num_iterations = 1000

for iteration in range(num_iterations):
    # ä¸¦è¡Œæ”¶é›† trajectories
    def policy_fn(params, obs):
        return policy.select_action(obs, deterministic=False)
    
    rollouts = pool.collect_rollouts(
        policy_fn=policy_fn,
        policy_params=policy.get_params(),
        num_rollouts=32,
        max_steps=1000
    )
    
    # å­˜å„² trajectories
    for rollout in rollouts:
        for t in range(len(rollout['observations'])):
            policy.store_transition(
                rollout['observations'][t],
                rollout['actions'][t],
                rollout['rewards'][t],
                rollout['observations'][t+1] if t < len(rollout['observations'])-1 
                    else rollout['observations'][t],
                rollout['dones'][t]
            )
    
    # PPO æ›´æ–°
    metrics = policy.update_step()
    
    # è¨˜éŒ„
    mean_reward = np.mean([r['episode_reward'] for r in rollouts])
    print(f"Iteration {iteration}: "
          f"Mean Reward={mean_reward:.2f}, "
          f"Loss={metrics['loss']:.4f}")

pool.close_all()
```

### ç¤ºä¾‹ 3: è‡ªå®šç¾©ç’°å¢ƒå’Œè¨“ç·´æ–¹æ³•

```python
"""
ä½¿ç”¨è‡ªå®šç¾©ç’°å¢ƒå’Œè‡ªå®šç¾©è¨“ç·´æ–¹æ³•
"""

# 1. å‰µå»ºè‡ªå®šç¾©ç’°å¢ƒ
import gymnasium as gym

class MyCustomEnv(gym.Env):
    def __init__(self):
        super().__init__()
        self.observation_space = gym.spaces.Box(
            low=-1, high=1, shape=(10,)
        )
        self.action_space = gym.spaces.Box(
            low=-1, high=1, shape=(2,)
        )
    
    def reset(self, seed=None):
        # ä½ çš„å¯¦ç¾
        obs = np.zeros(10)
        return obs, {}
    
    def step(self, action):
        # ä½ çš„å¯¦ç¾
        obs = np.zeros(10)
        reward = 0.0
        terminated = False
        truncated = False
        return obs, reward, terminated, truncated, {}

# è¨»å†Šç’°å¢ƒ
gym.register(
    id='MyCustomEnv-v0',
    entry_point=MyCustomEnv
)

# 2. ä½¿ç”¨ç’°å¢ƒ
env = GymnasiumEnvWrapper('MyCustomEnv-v0')

# 3. ä½¿ç”¨ä½ çš„è‡ªå®šç¾©è¨“ç·´æ–¹æ³•
policy = create_policy_for_environment(
    architecture=architecture,
    env_info=env.get_env_info(),
    method='my_method',  # ä½ ä¹‹å‰è¨»å†Šçš„æ–¹æ³•
    config=config
)

# 4. è¨“ç·´
# ... (training loop) ...
```

## ğŸ“š API åƒè€ƒ

### ArchitectureSpec

```python
ArchitectureSpec(
    nodes: jnp.ndarray,
    connections: jnp.ndarray,
    num_inputs: int,
    num_outputs: int,
    num_hidden: int,
    num_params: int,
    search_fitness: float,
    search_complexity: float,
    activation_functions: Dict[int, str] = {},
    metadata: Dict[str, Any] = {}
)

# æ–¹æ³•
spec.save(path: str)
spec = ArchitectureSpec.load(path: str)
```

### WANNArchitecture

```python
WANNArchitecture(
    spec: ArchitectureSpec,
    genome: Optional[WANNGenome] = None
)

# æ–¹æ³•
params = architecture.init_params(key: PRNGKey)
output = architecture.forward(x: Array, params: Dict)
num_params = architecture.get_num_params()
info = architecture.get_architecture_info()
dict_data = architecture.to_dict()
architecture = WANNArchitecture.from_dict(dict_data)
```

### TrainingConfig

```python
TrainingConfig(
    learning_rate: float = 3e-4,
    batch_size: int = 256,
    buffer_size: int = 100000,
    num_epochs: int = 1000,
    eval_frequency: int = 10,
    eval_episodes: int = 10,
    checkpoint_frequency: int = 50,
    checkpoint_dir: str = "./checkpoints",
    log_frequency: int = 1,
    method_kwargs: Dict[str, Any] = {}
)
```

### GymnasiumEnvWrapper

```python
GymnasiumEnvWrapper(
    env_name: str,
    render_mode: Optional[str] = None,
    **env_kwargs
)

# æ–¹æ³•
obs, info = env.reset(seed: Optional[int])
obs, reward, terminated, truncated, info = env.step(action: Array)
info = env.get_env_info()
env.close()
```

### DistributedEnvPool

```python
DistributedEnvPool(
    env_name: str,
    num_workers: int = 4,
    **env_kwargs
)

# æ–¹æ³•
rollouts = pool.collect_rollouts(
    policy_fn: Callable,
    policy_params: Dict,
    num_rollouts: int,
    max_steps: int = 1000
)
pool.reset_all()
pool.close_all()
info = pool.get_env_info()
```

### TrainingMethodRegistry

```python
# è¨»å†Šæ–°æ–¹æ³•
@TrainingMethodRegistry.register('method_name')
class MyMethod(PolicyInterface):
    ...

# ç²å–æ–¹æ³•
method_cls = TrainingMethodRegistry.get('method_name')

# åˆ—å‡ºæ‰€æœ‰æ–¹æ³•
methods = TrainingMethodRegistry.list_methods()
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. JAX OOM

```python
# æ¸›å°‘æ‰¹æ¬¡å¤§å°
config = TrainingConfig(batch_size=64)

# æˆ–ä½¿ç”¨ CPU
import jax
jax.config.update('jax_platform_name', 'cpu')
```

#### 2. Ray åˆå§‹åŒ–éŒ¯èª¤

```python
# ç¢ºä¿ Ray æœªåˆå§‹åŒ–
import ray
if ray.is_initialized():
    ray.shutdown()

# é‡æ–°åˆå§‹åŒ–
ray.init(ignore_reinit_error=True)
```

#### 3. ç’°å¢ƒå…¼å®¹æ€§

```python
# æ¸¬è©¦ç’°å¢ƒ
from wann_sdk_ray_env import test_environment
test_environment("YourEnv-v0", num_episodes=3)
```

## ğŸ“ æœ€ä½³å¯¦è¸

### 1. æ¶æ§‹æœç´¢åƒæ•¸èª¿å„ª

```python
# è¼ƒå°çš„ç¨®ç¾¤ç”¨æ–¼å¿«é€Ÿå¯¦é©—
WANN(pop_size=500, generations=50)

# è¼ƒå¤§çš„ç¨®ç¾¤ç”¨æ–¼æœ€çµ‚æœç´¢
WANN(pop_size=2000, generations=200)

# èª¿æ•´è¤‡é›œåº¦æ¬Šé‡
WANN(complexity_weight=0.3)  # æ›´åå¥½ç°¡å–®ç¶²çµ¡
```

### 2. è¨“ç·´ç©©å®šæ€§

```python
# ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦
import optax
schedule = optax.exponential_decay(
    init_value=3e-4,
    transition_steps=10000,
    decay_rate=0.99
)
optimizer = optax.adam(schedule)
```

### 3. æª¢æŸ¥é»ç®¡ç†

```python
# å®šæœŸä¿å­˜
config = TrainingConfig(
    checkpoint_frequency=10000,
    checkpoint_dir="./checkpoints"
)

# å¾æª¢æŸ¥é»æ¢å¾©
# ... (load checkpoint and resume) ...
```

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿è²¢ç»æ–°çš„è¨“ç·´æ–¹æ³•ã€ç’°å¢ƒåŒ…è£å™¨æˆ–æ”¹é€²ï¼

### æ·»åŠ æ–°è¨“ç·´æ–¹æ³•

1. å¯¦ç¾ `PolicyInterface`
2. ä½¿ç”¨ `@TrainingMethodRegistry.register` è¨»å†Š
3. æ·»åŠ æ¸¬è©¦
4. æ›´æ–°æ–‡æª”

### æ·»åŠ æ–°ç’°å¢ƒ

1. ç¹¼æ‰¿ `GymnasiumEnvWrapper`
2. å¯¦ç¾ç‰¹å®šç’°å¢ƒçš„é è™•ç†
3. æ·»åŠ æ¸¬è©¦
4. æ›´æ–°æ–‡æª”

## ğŸ“„ è¨±å¯è­‰

æœ¬ SDK éµå¾ªä»¥ä¸‹è¨±å¯è­‰:
- WANN: MIT License
- TensorNEAT: GPL-3.0
- EvoX: GPL-3.0

## ğŸ™ è‡´è¬

- Weight Agnostic Neural Networks (Gaier & Ha)
- TensorNEAT (EMI-Group)
- EvoX (EMI-Group)
- Gymnasium (Farama Foundation)
- Ray (Anyscale)

---

**Happy Training! ğŸš€**

å¦‚æœ‰å•é¡Œ,è«‹åƒè€ƒç¤ºä¾‹ä»£ç¢¼æˆ–æäº¤ Issueã€‚
